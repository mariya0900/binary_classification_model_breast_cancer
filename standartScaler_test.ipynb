{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data after scaling:\n",
      " [[-1.22474487 -1.22474487]\n",
      " [ 0.          0.        ]\n",
      " [ 1.22474487  1.22474487]]\n",
      "train mean: \n",
      " [0. 0.]\n",
      "train standart deviation: \n",
      " [1. 1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# create a toy dataset with two features\n",
    "X = np.array([[1, 2], [3, 4], [5, 6]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (original data) mean: \n",
      " [3. 4.]\n",
      "X (original data) standart deviation: \n",
      " [1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"X (original data) mean: \\n\", np.mean(X, axis = 0))\n",
    "print(\"X (original data) standart deviation: \\n\", np.std(X_train, axis= 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandartScaler mean: \n",
      " [3. 4.]\n"
     ]
    }
   ],
   "source": [
    "# create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X)\n",
    "print(\"StandartScaler mean: \\n\", scaler.mean_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data after scaling:\n",
      " [[-1.22474487 -1.22474487]\n",
      " [ 0.          0.        ]\n",
      " [ 1.22474487  1.22474487]]\n",
      "train mean: \n",
      " [0. 0.]\n",
      "train standart deviation: \n",
      " [1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# use fit_transform on the training data\n",
    "X_train = scaler.fit_transform(X)\n",
    "print(\"Training data after scaling:\\n\", X_train)\n",
    "print(\"train mean: \\n\", np.mean(X_train, axis = 0))\n",
    "print(\"train standart deviation: \\n\", np.std(X_train, axis= 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data after scaling:\n",
      " [[2.44948974 2.44948974]\n",
      " [3.67423461 3.67423461]\n",
      " [4.89897949 5.51135192]]\n",
      "test mean: \n",
      " [ 9.         10.33333333]\n",
      "test standart deviation: \n",
      " [1.63299316 2.05480467]\n"
     ]
    }
   ],
   "source": [
    "# use transform on the testing data\n",
    "X_test = np.array([[7, 8], [9, 10], [11, 13]])\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"Testing data after scaling:\\n\", X_test_scaled)\n",
    "print(\"test mean: \\n\", np.mean(X_test, axis = 0))\n",
    "print(\"test standart deviation: \\n\", np.std(X_test, axis= 0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, test data mean and standart variation are not [0 0] and [1 1] respectively, like the train data was. This is because the mean and std were computed on the train data. \n",
    "So, we only fit StandartScaler on the training data. By doing this, we ensure that our model isn't overfitting to the testing data, and that it's being evaluated only on unseen data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
